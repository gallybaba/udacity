{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartCab Solution Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smartcab eventually reaches its destination but takes a longer time. Also notice that it does not stop on red lights sometimes. Thirdly, as expected, it is not learning from past mistakes. With DeadLine NOT enforced, it misses trial and does not reach destination in 100 attempts or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State I chose is a combination of Light, Oncoming and Next WayPoint. The selection of state seem reasonable as smarcab reaches destination with given constraints.\n",
    "__Light__ This is important as a basic fundamental rule of driving is to follow light signal. In this case there are two choices, True and False meaning if it is Green or Red.\n",
    "__OnComing__ This state tells us whether there is oncoming traffic or not. It is also important as cab can turn right on a red light but needs to watch oncoming traffic. Other way it is important is Oncoming traffic gets a priority when light is green and cab needs to turn left.\n",
    "__NextWayPoint__ where to go from here? This state sums the last two states and defines the next action. Once the action is acted upon, a reward is assessed. Without this state, cab will only learn half the conditions and may start circling around the block.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total States: Total Locations * Total Light States * Total Oncoming * Total Actions\n",
    "Total Locations: 48 (8*6 grid)\n",
    "Total Light: 2 (True and False)\n",
    "Total Oncoming: 2 (True and False)\n",
    "Total Actions: 4 (None, Forward, left, Right)\n",
    "Total States = 48 * 2 * 2 * 4\n",
    "Total States: 768\n",
    "\n",
    "We do not need Location on Q Learning states as it will take a very long time learn. Although, getting a complete state picture is ideal but is not possible in a limmited number of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed three important changes. 1) When picking actions from Q matrix, agent still explores a little bit. Upon exploration if it finds a low reward and when it comes across a similar situation, it picks a different route until it knows the best action to take based on rewards and Q policy. 2) Agent updates Q matrix incrementally untill all actions are explored. This makes every subsequent trial likely to be better than last ones. 3) Once converged, agent picks up the right path quickly and reaches to destination almost every single trial within the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Best Combination__* Agent Performed 80+ successful trials for its best trials with a minimum penalty of 26. On an average, across all combinations, agent was successfull 70+ trials with penalties ranging from 26 - 200.\n",
    "   * gamma = 1.0\n",
    "   * alpha = 1.0\n",
    "   * epsilon = 0.0\n",
    "*__Other Combinations__*\n",
    "   * gamma = 0.9, alpha = 0.6, epsilon = 0.0\n",
    "   * gamma = 0.8, alpha = 0.7, epsilon = 0.2\n",
    "   * gamma = 0.8, alpha = 0.6, epsilon = 0.2\n",
    "   * gamma = 0.6, alpha = 0.3, epsilon = 0.4\n",
    "   * gamma = 0.4, alpha = 0.3, epsilon = 0.3\n",
    "   * gamma = 0.7, alpha = 0.6, epsilon = 0.3\n",
    "   * gamma = 0.7, alpha = 0.6, epsilon = 0.2\n",
    "   * gamma = 0.9, alpha = 0.7, epsilon = 0.2\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No agent does NOT find an optimum policy. Best the agent can do is 80+ successfull trials with minimum penalty (26). On an average with all combinations of parameters agent made 70+ successfull trials with penalty ranging from 20 - 200. Max successful trials was 100 but with penalties reaching 92."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
