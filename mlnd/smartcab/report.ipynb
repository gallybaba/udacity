{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartCab Solution Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smartcab eventually reaches its destination but takes a longer time. Also notice that it does not stop on red lights sometimes. Thirdly, as expected, it is not learning from past mistakes. With DeadLine NOT enforced, it misses trial and does not reach destination in 100 attempts or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State I chose is a combination of Light, Oncoming and Next WayPoint. The selection of state seem reasonable as smarcab reaches destination with given constraints.\n",
    "__Light__ This is important as a basic fundamental rule of driving is to follow light signal. In this case there are two choices, True and False meaning if it is Green or Red.\n",
    "__OnComing__ This state tells us whether there is oncoming traffic or not. It is also important as cab can turn right on a red light but needs to watch oncoming traffic. Other way it is important is Oncoming traffic gets a priority when light is green and cab needs to turn left.\n",
    "__NextWayPoint__ where to go from here? This state sums the last two states and defines the next action. Once the action is acted upon, a reward is assessed. Without this state, cab will only learn half the conditions and may start circling around the block. \n",
    "__Left__: This state tells the direction left traffic is turning toward. There is a situation when rewards are affected from the left traffic. If left traffic is coming forward, and agent decides to turn right on a red light, there is a chance of collision. The agent needs to remember to yield to oncoming traffic from left. If not yielded, agent should be penalized with negative rewards whereas turning right when left is not coming forward should be rewarded. Learning from not yielding to traffic coming from left is simple and cheap. Hence, adding it from set of states in Q matrix is justified and required.\n",
    "\n",
    "* Left Out States: *\n",
    "There are two states left out from the choice of states. 1) __Right__: Right traffic is controlled by lights and is not important to learn.\n",
    "2) __Deadline__: Influenced by deadline, an agent would be rushed into reaching destination not minding penalties. Deadline is subjective and can be realistic or non realistic. Learning an optimum policy should be exclusive of deadline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total States: Total Locations * Total Light States * Total Oncoming * Total Actions * Left * Right * Next Waypoint\n",
    "Total Locations: 48 (8*6 grid)\n",
    "Total Light: 2 (True and False)\n",
    "Total Oncoming: 4 (None, Forward, Left, Right)\n",
    "Total Actions: 4 (None, Forward, left, Right)\n",
    "Total Left: 4 (None, Forward, Left, Right)\n",
    "Total Right: 4 (None, Forward, Left, Right)\n",
    "Total States = 48 * 2 * 4 * 4 * 4 * 4\n",
    "Total States: 24576\n",
    "\n",
    "We do not need Location on Q Learning states as it will take a very long time learn. Although, getting a complete state picture is ideal but is not possible in a limmited number of trials. Similarly, Left and Right are not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed three important changes. 1) When picking actions from Q matrix, agent still explores a little bit. Upon exploration if it finds a low reward and when it comes across a similar situation, it picks a different route until it knows the best action to take based on rewards and Q policy. 2) Agent updates Q matrix incrementally untill all actions are explored. This makes every subsequent trial likely to be better than last ones. 3) Once converged, agent picks up the right path quickly and reaches to destination almost every single trial within the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Best Combination__* Agent Performed 80+ successful trials for its best trials with a minimum penalty of 26. On an average, across all combinations, agent was successfull 70+ trials with penalties ranging from 26 - 200.\n",
    "   * gamma = 1.0\n",
    "   * alpha = 1.0\n",
    "   * epsilon = 0.0\n",
    "*__Other Combinations__*\n",
    "   * gamma = 0.9, alpha = 0.6, epsilon = 0.0\n",
    "   * gamma = 0.8, alpha = 0.7, epsilon = 0.2\n",
    "   * gamma = 0.8, alpha = 0.6, epsilon = 0.2\n",
    "   * gamma = 0.6, alpha = 0.3, epsilon = 0.4\n",
    "   * gamma = 0.4, alpha = 0.3, epsilon = 0.3\n",
    "   * gamma = 0.7, alpha = 0.6, epsilon = 0.3\n",
    "   * gamma = 0.7, alpha = 0.6, epsilon = 0.2\n",
    "   * gamma = 0.9, alpha = 0.7, epsilon = 0.2\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__So what does an optimal policy look like?__*\n",
    "   * does it look like the fastest time to reach destination? No.\n",
    "   * does it look like zero errors? Yes but not just that.\n",
    "   * It looks like it can reach destination without errors following ALL rules and no collisions AND reach destination most of times within a threshold. Ideally, assuming I am driving and I know the way, I can reach that destination in an average traffic condition in say 30 mins. I would like the smartcab to reach its destination in or around the same times (+- 10%) but with NO traffic violations.\n",
    "*__Does our agent reach optimal policy__*\n",
    "No agent does NOT find an optimum policy but is close. Best the agent can do is 95+ successfull trials with minimum penalty (26). On an average with all combinations of parameters agent made 70+ successfull trials with penalty ranging from 20 - 200. Max successful trials was 100 but with penalties reaching 52.\n",
    "Agent is, however, making progress and is converging to a stable Q matrix.\n",
    "('red', None, 'left')          : [0.0, -1.0, -1.0, -0.5], \n",
    "('red', None, 'right')         : [0  , 0   , 0   ,  2.0], \n",
    "('red', 'left', 'left')        : [0.0, -1.0, -1.0,    0], \n",
    "('green', None, 'left')        : [0.0, 0   , 2.0 ,    0], \n",
    "('red', 'left', 'right')       : [0.0, -1.0, -1.0,    0], \n",
    "('green', None, None)          : [0  , 0   , 0   ,    0], \n",
    "('red', None, None)            : [0  , 0   , 0   ,    0], \n",
    "('green', 'left', 'forward')   : [0  , 4.0 , 0   ,    0], \n",
    "('green', 'left', 'right')     : [0.0, 0   , 0   , 12.0], \n",
    "('red', None, 'forward')       : [0.0, -1.0,-1.0 , -0.5], \n",
    "('green', 'right', 'forward')  : [0  , 0, 0      , -0.5], \n",
    "('red', 'left', 'forward')     : [0.0, -1.0,-1.0 ,  1.5], \n",
    "('red', 'forward', 'forward')  : [0.0, 0, 0      ,    0], \n",
    "('red', 'forward', 'right')    : [0.0, 0, 0      , 14.0], \n",
    "('red', 'right', 'forward')    : [0  , 0   ,-1.0 , 13.5], \n",
    "('green', 'forward', 'forward'): [0.0, 0   , 0   ,    0], \n",
    "('green', None, 'right')       : [0.0, 0   , 0   , 14.0], \n",
    "('green', None, 'forward')     : [0  , 12.0, 0   , -0.5], \n",
    "('red', 'forward', None)       : [0  , 0   , 0   ,   0]}, \n",
    "We can see that Q matrix is converging although not fully converged yet. That is evidence that policy is gearing towards optimum state.\n",
    "Although we are making progress, we see wrong actions, hence negative rewards. For e.g.\n",
    "__current_negative_reward= ['Wrong Action: Light: green, Oncoming: None, NextWaypoint: forward, action: right, Reward: -0.5']__\n",
    "In this case, There is no oncoming and light is green but instead of moving to the next waypoint (forward), it moves in some other direction (right). To that affect, it enjoys the negative reward (-0.5).\n",
    "__current_negative_reward= ['Wrong Action: Light: red, Oncoming: None, NextWaypoint: left, action: right, Reward: -0.5']__\n",
    "In this case, agent notices a red light so it does not take a left, but it takes a right instead. It probably should have done nothing.\n",
    "So we can see how the agent is learning and makes to the destination but with some penalties. In this way, learning through penalties and positive rewards. The policy is gearing towards optimality but yet to reach *zero* error state. It is also evident that once the Q matrix is fully converged, our desired Optimal policy should be pleasured."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
